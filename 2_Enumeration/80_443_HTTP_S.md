* [Inspect HTML Code](#Inspect-HTML-Code-of-Webpapges)
* [Evaluate IP vs. Hostname Resolution](#Evaluate-IP-vs-Hostname-Resolution)
* [Evaluate HTTP/S Usage](#Evaluate-HTTPS-Usage)
* [Evaluate Code Support Beyond HTML](#Evaluate-Code-Support-Beyond-HTML-ippsecFriendZone)
* [Evaluate Robots.txt File](#Evaluate-Robotstxt-File-ippsecIrked)
* [Web Directory Enumeration](#Web-Directory-Enumeration)
* [OS Fingerprinting](#OS-Fingerprinting-ippsecShcoker)
- Content Management Systems (CMS) Enumeration
  * [CMS Explorer](#CMS-Explorer)
  * [Wordpress](#Wordpress)
  * [Joomscan](#Joomscan)
* [Manual Method Testing](#Manual-Method-Testing)
* [Javascript Framework](#Javascript-Framework-ippsecNode)
* [Cookie Inspection](#Cookie-Inspection-VulnHub-Temple-of-Doom)
- HTTPS-Specific
  * [Evaluate for Heartbleed](#Evaluate-for-Heartbleed)
  * [Evaluate SSL Cert Details](#Evaluate-SSL-Cert-Details)



# Inspect HTML Code of Webpapges
- Should be done on each page you are able to navigate to.
- Focus on where there are any usernames / passwords, comments / notes, extraneous / non-prod code, or hidden values in the HTML.

# Evaluate IP vs. Hostname Resolution
- Sometimes hostname-based routing is applied in the web server service in which you get different results if you navigate to port 80 with the IP or the hostname. 
- Try both IP and Hostname to see if this is the case to capture all enumeration possibilities. 
1. Identify the hostname of the box – generally through the 'CommonName' field of the SSL cert for web pages with HTTPS running (generally over port 443). 
2. Add the mapping between IP and hostname into the attacker machine host file. 
```
vi /etc/hosts/
[IP address]         [hostname] 
```
3. Access the webpage through the IP address over both port 80 and 443. 
4. Access the webpage through the host name over both port 80 and 443. 
- If they both result in the same page, then there is no hostname-based routing applicable.

# Evaluate HTTP/S Usage
- Generally HTTP works over port 80 and HTTPS works over port 443.  However, this isn't always the case.  Sometimes there is different content served over different protocols through either of these ports. 
1. Access port 80 over HTTP and HTTPS protocols. 
2. Access port 443 over HTTP and HTTPS protocols

# Evaluate SSL Cert Details
- Sites that allow SSL/TLS over HTTPS have a Public Certificate that is viewable by all clients. 
1. Navigate to the webpage using the HTTPS protocol. 
2. Click on the lock in the address bar. 
3. More Information > Security > View Certificate > Details 
4. Hostname > Certificate > Issuer 
  - Email address with a username in the 'E' field. 
  - Hostname in the 'CN' field. 
5. Hostname > Certificate > Subject 
  - Email address with a username in the 'E' field. 
  - Hostname in the 'CN' field.

# Evaluate Code Support Beyond HTML (ippsec:FriendZone)
1. Try to append /index.html and /index.php to the end of the IP address of the web server.
2. If index.html resolves but index.php doesn't, then the website likely doesn't support PHP and is likely just static HTML.  To speed up recon, the web pages are unlikely to have significant attack surface.  If .php does resovle, it's worth further enumeration of the website.

# Evaluate Robots.txt File (ippsec=Irked)
- This is included at the root of the web server address to indicate which pages of the website not to be indexed by search engines like Google.
- If it exists, it usually discloses pages, directories, and extensions that the web site creator didn't want the general public knowing about through using a search engine.
```
http://[IP / Hostname]:[port]/robots.txt
```

# Evaluate for Heartbleed
- OpenSSL 1.0.1 through 1.0.1f (inclusive) are vulnerable.
- OpenSSL 1.0.1g is NOT vulnerable.
- OpenSSL 1.0.0 branch is NOT vulnerable.
- OpenSSL 0.9.8 branch is NOT vulnerable.
```
sslscan [IP]:443
```
```
nmap -sV --script=ssl-heartbleed [IP]
```
```
sslyze -h [IP] --heartbleed
```

# Web Directory Enumeration
You can run this on the the top-level IP / host name or on a sub-directory/folder of interest.  Generally these sub-directories / folders of interest are the ones that show up in the robots.txt.
```
gobuster dir -u http(s)://[IP Address / Hostname]/[file / folder] -w "[wordlist location options below]" –o "[output file location]" -a "Mozilla/5.0 (X11; Linux x86_64; rv:60.0) Gecko/20100101 Firefox/60.0" -e -k -l -s "200,204,301,302,307,401,403" -x "txt,html,php,asp,aspx,jsp"
```
- /usr/share/seclists/Discovery/Web_Content/common.txt
- /usr/share/wordlists/dirbuster/directory-list-lowercase-2.3-medium.txt
- If you find a number of scripts in the results, try checking in /cgi-bin/.  This is an Apache directory which gives content from the web server over to a different internal scripting language for furhter processing. (ippsec:Shocker)

# OS Fingerprinting (ippsec:Shcoker)
Works best with Apache web servers as there are specific versions for different flavors of \*nix.
1. Get the web server service version from an Nmap service scan.  Syntax in this output is Service : Web Server Application : Web Server App Version
2. Google / use launchpad.net to identify the \*nix OS version that works with the web server version.
or
2. Go to packages.ubuntu.com, search the web server application, then cycle through the distro flavors to identify which web application pacakge version number matches the one from the HTTP service scan.  This will only fingerprint the OS distribution (not the patch level).

# Content Management Systems (CMS) Enumeration

## CMS Explorer
Used for initial fingerprinting of what CMS are used on the site.
```
cms-explorer -url http://INSERTIPADDRESS -type [Drupal, WordPress, Joomla, Mambo]
```

## Wordpress
- Wordpress usually references files using absolute paths (the full path name) instead of relative paths (just the file within the current web server directory).  We can use this behavior to view the source of webpages to identify full web server directory structure.
```
wpscan --url http://INSERTIPADDRESS --enumerate vp 
wpscan --url http://INSERTIPADDRESS --enumerate vt 
wpscan --url http://INSERTIPADDRESS --enumerate u
```
- vp: Vulnerable Plugins
- vt: Vulnerable Themes
- u: Users

## Joomscan
```
joomscan -u  http://INSERTIPADDRESS --enumerate-components
```

# Manual Method Testing
- Used to understand what methods are allowed and not allowed.
- This is generally taken care of by basic Nmap script scanning of HTTP(s) pages.
```
nc [IP] [port]
HEAD / HTTP/1.0  
OPTIONS / HTTP/1.0  
PROPFIND / HTTP/1.0  
TRACE / HTTP/1.1  
PUT http://[Target_URL]/[FILE_NAME]
POST http://[Target_URL]/[FILE_NAME] HTTP/1.x  
```

# Javascript Framework (ippsec=Node)

## Known JS Files
- If Javascript framework is used by the web application, then we can navigate to a known .js file to get a listing of connected resource pages for further analysis.
```
http://[IP]:[port]/Host/assets/js/app/app.js
```
  - You can to look at the response of this request through a web proxy to see the routes of the web application.
  - You can also navigate to this file through a web proxy site map of the web application.
  
# Cookie Inspection (VulnHub: Temple of Doom)
- Cookies that are set by the browser could be encoded with valuable information.
- Note that this is common with Node.js websites as they typically serialize JSON-formatted data (cookies) through Base64 then URL encoding before sending it back to the browser client as a cookie.  If you're target uses JS / Node.js - this is a likely attack vector.  RCE of Node.js deserialization can be found here:
 - [Node.js Deserialization](https://opsecx.com/index.php/2017/02/08/exploiting-node-js-deserialization-bug-for-remote-code-execution/)
 ```
 {"username":"_$$ND_FUNC$$_function (){return require('child_process').execSync('ls /', function(error, stdout, stderr) { console.log(stdout) });\n }()"}
 ```
  - Change the username field to whatever JSON field is accepted in what's passed through the cookie.
1. Look at the cookie value set by the browser when you navigate to a site.
2. See if it changes upon new navigation to the site (incognito browsing / repeater requests).
3. If it doesn't change, then it is likely an encoded secret.  If it does change, it's still worth looking into if we haven't found anything else in our enumeration.
4. You can copy the cookie value and try Burp Decoder with it to see if decoding the value to something else (potentially through multiple iterations) could result in anything usable.  Common encoding patterns to try during decoding is URL, Base64, and Gzip.  Try a combination of multiple.  A few tips:
 - If the value has % signs in it, then it's likely URL encoded.
 - If the value has = signs in it (especially if it ends with them), then it's likely Base64 encoded.
